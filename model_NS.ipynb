{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Book-Oracle: Basic Recommendation System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Develop a basic Recommendation System\n",
    "- 26.11.2023\n",
    "- Janina, Oliwia, Neha, Nina"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Modelling\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, OneHotEncoder\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, roc_curve, confusion_matrix, make_scorer, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "#NLP\n",
    "import nltk\n",
    "\n",
    "#Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "plt.rcParams.update({ \"figure.figsize\" : (8, 5),\"axes.facecolor\" : \"white\", \"axes.edgecolor\":  \"black\"})\n",
    "plt.rcParams[\"figure.facecolor\"]= \"w\"\n",
    "pd.plotting.register_matplotlib_converters()\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "pd.options.display.float_format = \"{:,.2f}\".format\n",
    "\n",
    "RSEED = 42\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1005487, 12)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/kaggle_full_df.csv')\n",
    "df['country'].fillna('unknown', inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(290837, 12)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Only Rating greater than 4\n",
    "df = df[df['book_rating']>4]\n",
    "\n",
    "#Only users from US or Canada\n",
    "df = df[df['country'].str.contains(\"usa|canada\")]\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_title</th>\n",
       "      <th>book_author</th>\n",
       "      <th>year_of_publication</th>\n",
       "      <th>publisher</th>\n",
       "      <th>image_url_m</th>\n",
       "      <th>common_identifier</th>\n",
       "      <th>user_id</th>\n",
       "      <th>isbn</th>\n",
       "      <th>book_rating</th>\n",
       "      <th>age</th>\n",
       "      <th>city</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Classical Mythology</td>\n",
       "      <td>Mark P. O. Morford</td>\n",
       "      <td>2002</td>\n",
       "      <td>Oxford University Press</td>\n",
       "      <td>http://images.amazon.com/images/P/0195153448.0...</td>\n",
       "      <td>1</td>\n",
       "      <td>269782</td>\n",
       "      <td>0801319536</td>\n",
       "      <td>7</td>\n",
       "      <td>30</td>\n",
       "      <td>edmonton</td>\n",
       "      <td>canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pay It Forward: A Novel</td>\n",
       "      <td>Catherine Ryan Hyde</td>\n",
       "      <td>2000</td>\n",
       "      <td>Simon &amp;amp; Schuster</td>\n",
       "      <td>http://images.amazon.com/images/P/0684862719.0...</td>\n",
       "      <td>2392</td>\n",
       "      <td>269782</td>\n",
       "      <td>0684862719</td>\n",
       "      <td>8</td>\n",
       "      <td>30</td>\n",
       "      <td>edmonton</td>\n",
       "      <td>canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Watership Down</td>\n",
       "      <td>Richard Adams</td>\n",
       "      <td>1976</td>\n",
       "      <td>Avon</td>\n",
       "      <td>http://images.amazon.com/images/P/0380002930.0...</td>\n",
       "      <td>3172</td>\n",
       "      <td>269782</td>\n",
       "      <td>0140039589</td>\n",
       "      <td>10</td>\n",
       "      <td>30</td>\n",
       "      <td>edmonton</td>\n",
       "      <td>canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Writing The Circle: Native Women Of Western Ca...</td>\n",
       "      <td>Jeanne Perreault</td>\n",
       "      <td>1990</td>\n",
       "      <td>Lpg Distribution</td>\n",
       "      <td>http://images.amazon.com/images/P/0920897886.0...</td>\n",
       "      <td>95231</td>\n",
       "      <td>269782</td>\n",
       "      <td>0920897886</td>\n",
       "      <td>10</td>\n",
       "      <td>30</td>\n",
       "      <td>edmonton</td>\n",
       "      <td>canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Clara Callan</td>\n",
       "      <td>Richard Bruce Wright</td>\n",
       "      <td>2001</td>\n",
       "      <td>HarperFlamingo Canada</td>\n",
       "      <td>http://images.amazon.com/images/P/0002005018.0...</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0002005018</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>timmins</td>\n",
       "      <td>canada</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          book_title           book_author  \\\n",
       "1                                Classical Mythology    Mark P. O. Morford   \n",
       "2                            Pay It Forward: A Novel   Catherine Ryan Hyde   \n",
       "3                                     Watership Down         Richard Adams   \n",
       "5  Writing The Circle: Native Women Of Western Ca...      Jeanne Perreault   \n",
       "6                                       Clara Callan  Richard Bruce Wright   \n",
       "\n",
       "  year_of_publication                publisher  \\\n",
       "1                2002  Oxford University Press   \n",
       "2                2000     Simon &amp; Schuster   \n",
       "3                1976                     Avon   \n",
       "5                1990         Lpg Distribution   \n",
       "6                2001    HarperFlamingo Canada   \n",
       "\n",
       "                                         image_url_m  common_identifier  \\\n",
       "1  http://images.amazon.com/images/P/0195153448.0...                  1   \n",
       "2  http://images.amazon.com/images/P/0684862719.0...               2392   \n",
       "3  http://images.amazon.com/images/P/0380002930.0...               3172   \n",
       "5  http://images.amazon.com/images/P/0920897886.0...              95231   \n",
       "6  http://images.amazon.com/images/P/0002005018.0...                  2   \n",
       "\n",
       "   user_id        isbn  book_rating  age      city country  \n",
       "1   269782  0801319536            7   30  edmonton  canada  \n",
       "2   269782  0684862719            8   30  edmonton  canada  \n",
       "3   269782  0140039589           10   30  edmonton  canada  \n",
       "5   269782  0920897886           10   30  edmonton  canada  \n",
       "6        8  0002005018            5    0   timmins  canada  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_title</th>\n",
       "      <th>book_author</th>\n",
       "      <th>rating_count</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>83609</th>\n",
       "      <td>The Lovely Bones: A Novel</td>\n",
       "      <td>Alice Sebold</td>\n",
       "      <td>601</td>\n",
       "      <td>601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77011</th>\n",
       "      <td>The Da Vinci Code</td>\n",
       "      <td>Dan Brown</td>\n",
       "      <td>411</td>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88787</th>\n",
       "      <td>The Secret Life Of Bees</td>\n",
       "      <td>Sue Monk Kidd</td>\n",
       "      <td>379</td>\n",
       "      <td>379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87707</th>\n",
       "      <td>The Red Tent (Bestselling Backlist)</td>\n",
       "      <td>Anita Diamant</td>\n",
       "      <td>341</td>\n",
       "      <td>341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85091</th>\n",
       "      <td>The Nanny Diaries: A Novel</td>\n",
       "      <td>Emma Mclaughlin</td>\n",
       "      <td>330</td>\n",
       "      <td>330</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                book_title      book_author  rating_count  \\\n",
       "83609            The Lovely Bones: A Novel     Alice Sebold           601   \n",
       "77011                    The Da Vinci Code        Dan Brown           411   \n",
       "88787              The Secret Life Of Bees    Sue Monk Kidd           379   \n",
       "87707  The Red Tent (Bestselling Backlist)    Anita Diamant           341   \n",
       "85091           The Nanny Diaries: A Novel  Emma Mclaughlin           330   \n",
       "\n",
       "       Count  \n",
       "83609    601  \n",
       "77011    411  \n",
       "88787    379  \n",
       "87707    341  \n",
       "85091    330  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Add a new column with a total rating count for each book by common identifier\n",
    "df['rating_count'] = df.groupby(['book_title', 'book_author'])['book_rating'].transform('count')\n",
    "\n",
    "#Show a list of books that got the highest rating count, group by title and author to show unique books\n",
    "\n",
    "df.groupby(['book_title', 'book_author', 'rating_count']).size().reset_index(name='Count').sort_values(by='rating_count', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49104, 13)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "popularity_threshold = 50\n",
    "df = df[df['rating_count'] >= popularity_threshold]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: White;\">Collaborative Filtering - Item based:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-surprise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Library used - Surprise\n",
    "\n",
    "- Model - matrix factorization SVD\n",
    "\n",
    "- Recommend top 5 books for a user. (Here, user_id is to be given as input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Identify books the user hasn't interacted with.\n",
    "- Make predictions for these books.\n",
    "- Sort predictions by estimated rating.\n",
    "- Extract the top N recommendations and return their titles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse: ##\n",
    "1. Understand the code\n",
    "\n",
    "2. What does RMSE mean in our model and why is it used?\n",
    "--> measure errors between predicted and actual values\n",
    "--> RSME is widely used in collaborative filtering \n",
    "--> Lower the RSME, better is the Model\n",
    "\n",
    "3. How is the train/test split done?\n",
    "--> train/test split is done on the \"data\"\n",
    "--> Total number of rows (Number of Ratings) in \"data\" object on which train/test split is done  :  1005487\n",
    "--> Number of unique users: 90976\n",
    "--> Number of items (common_identifiers): 245238\n",
    " \n",
    "\n",
    "4. Try the same with different libraries & include implicit rating as well \n",
    "    1. hybrid model from LightFM library \n",
    "    2. Alternating Least Squares(ALS) model from implicit's library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color: green;\"> Recommend top 5 books to User based on the books NOT interacted with </span>\n",
    "\n",
    "- Model used - Matrix Factorization SVD\n",
    "- Library used - Surprise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from surprise import Dataset, Reader\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import SVD\n",
    "from surprise import accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load data into the Surprise library's format\n",
    "reader = Reader(rating_scale=(4, 10))\n",
    "#reader = Reader(rating_scale=(1, 10))\n",
    "data = Dataset.load_from_df(df[['user_id', 'common_identifier', 'book_rating']], reader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<surprise.prediction_algorithms.matrix_factorization.SVD at 0x2a0ee0610>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Split the data into training and testing sets\n",
    "trainset, testset = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build and train the matrix factorization model\n",
    "model = SVD()\n",
    "model.fit(trainset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.3888\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.3887876041957476"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Make predictions on the test set\n",
    "predictions = model.test(testset)\n",
    "\n",
    "# Evaluate the model's accuracy\n",
    "accuracy.rmse(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to get book recommendations for a user\n",
    "def get_book_recommendations(user_id, df, model, n=5):\n",
    "    # Get the unique books the user hasn't interacted with\n",
    "    books_not_interacted = df[~df['common_identifier'].isin(df[df['user_id'] == user_id]['common_identifier'].tolist())]['common_identifier'].unique()\n",
    "    #books_not_interacted = df[~df['common_identifier'].isin(df[df['user_id'] == user_id]['common_identifier'].tolist())]['common_identifier'].unique()\n",
    "\n",
    "    # Make predictions for the books the user hasn't interacted with\n",
    "    predictions = [model.predict(user_id, book) for book in books_not_interacted]\n",
    "\n",
    "    # Sort predictions by estimated rating in descending order\n",
    "    sorted_predictions = sorted(predictions, key=lambda x: x.est, reverse=True)\n",
    "\n",
    "    # Get the top N recommendations\n",
    "    top_n_recommendations = sorted_predictions[:n]\n",
    "\n",
    "    # Extract book titles from recommendations\n",
    "    recommended_books = [df[df['common_identifier'] == prediction.iid]['book_title'].values[0] for prediction in top_n_recommendations]\n",
    "\n",
    "    return recommended_books\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 recommendations for user 270713: are as below\n",
      "1. The Stand: Complete And Uncut\n",
      "2. The Return Of The King (The Lord Of The Rings, Part 3)\n",
      "3. Lonesome Dove\n",
      "4. Where The Red Fern Grows\n",
      "5. The Fellowship Of The Ring (The Lord Of The Rings, Part 1)\n"
     ]
    }
   ],
   "source": [
    "# Call f(get_book_recommendations) for recommendatios\n",
    "user_id = 270713\n",
    "recommendations = get_book_recommendations(user_id, df, model, n=5)\n",
    "print(f\"Top 5 recommendations for user {user_id}: are as below\")\n",
    "for i, title in enumerate(recommendations[:5], start=1):\n",
    "    print(f\"{i}. {title}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.3894\n",
      "Top 5 recommendations for user 270713: are as below\n",
      "1. Dune (Remembering Tomorrow)\n",
      "2. The Return Of The King (The Lord Of The Rings, Part 3)\n",
      "3. The Stand: Complete And Uncut\n",
      "4. Fast Food Nation: The Dark Side Of The All-American Meal\n",
      "5. Charlotte'S Web (Trophy Newbery)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from surprise import Dataset, Reader\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import SVD\n",
    "from surprise import accuracy\n",
    "\n",
    "\n",
    "# Load data into the Surprise library's format\n",
    "reader = Reader(rating_scale=(1, 10))\n",
    "#reader = Reader(rating_scale=(1, 10))\n",
    "data = Dataset.load_from_df(df[['user_id', 'common_identifier', 'book_rating']], reader)\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "trainset, testset = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build and train the matrix factorization model\n",
    "model = SVD()\n",
    "model.fit(trainset)\n",
    "\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = model.test(testset)\n",
    "\n",
    "# Evaluate the model's accuracy\n",
    "accuracy.rmse(predictions)\n",
    "\n",
    "\n",
    "# Function to get book recommendations for a user\n",
    "def get_book_recommendations(user_id, df, model, n=5):\n",
    "    # Get the unique books the user hasn't interacted with\n",
    "    books_not_interacted = df[~df['common_identifier'].isin(df[df['user_id'] == user_id]['common_identifier'].tolist())]['common_identifier'].unique()\n",
    "    #books_not_interacted = df[~df['common_identifier'].isin(df[df['user_id'] == user_id]['common_identifier'].tolist())]['common_identifier'].unique()\n",
    "\n",
    "    # Make predictions for the books the user hasn't interacted with\n",
    "    predictions = [model.predict(user_id, book) for book in books_not_interacted]\n",
    "\n",
    "    # Sort predictions by estimated rating in descending order\n",
    "    sorted_predictions = sorted(predictions, key=lambda x: x.est, reverse=True)\n",
    "\n",
    "    # Get the top N recommendations\n",
    "    top_n_recommendations = sorted_predictions[:n]\n",
    "\n",
    "    # Extract book titles from recommendations\n",
    "    recommended_books = [df[df['common_identifier'] == prediction.iid]['book_title'].values[0] for prediction in top_n_recommendations]\n",
    "\n",
    "    return recommended_books\n",
    "\n",
    "\n",
    "# Call f(get_book_recommendations) for recommendatios\n",
    "user_id = 270713\n",
    "recommendations = get_book_recommendations(user_id, df, model, n=5)\n",
    "print(f\"Top 5 recommendations for user {user_id}: are as below\")\n",
    "for i, title in enumerate(recommendations[:5], start=1):\n",
    "    print(f\"{i}. {title}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  <span style=\"color: pink;\"> Accessing values from \"data\" object used in above code \n",
    "data = Dataset.load_from_df(df[['user_id', 'common_identifier', 'book_rating']], reader) </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output Format ----->   'user_id', 'common_identifier', 'book_rating']\n",
    "\n",
    "# Get the full training set from the data object\n",
    "full_trainset = data.build_full_trainset()\n",
    "\n",
    "# Convert the generator to a list and access the first few raw ratings\n",
    "raw_ratings = list(full_trainset.all_ratings())[:3]\n",
    "\n",
    "# Get the number of ratings (number of rows in the training set)\n",
    "num_ratings = full_trainset.n_ratings\n",
    "\n",
    "\n",
    "# Display the first few raw ratings\n",
    "print(raw_ratings)\n",
    "\n",
    "# Display the number of ratings\n",
    "print(\"Number of Ratings:\", num_ratings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#determine the number of unique users and items (common_identifiers) \n",
    "\n",
    "# Get the full training set from the data object\n",
    "full_trainset = data.build_full_trainset()\n",
    "\n",
    "# Get the number of users and items (common_identifiers)\n",
    "num_users = full_trainset.n_users\n",
    "num_items = full_trainset.n_items\n",
    "\n",
    "# Display the number of users and items\n",
    "print(\"Number of unique users:\", num_users)\n",
    "print(\"Number of items (common_identifiers):\", num_items)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lightfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install implicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from lightfm import LightFM\n",
    "from lightfm.data import Dataset\n",
    "from lightfm.cross_validation import random_train_test_split\n",
    "from lightfm.evaluation import precision_at_k\n",
    "\n",
    "# Create a LightFM dataset\n",
    "dataset = Dataset()\n",
    "dataset.fit(users=df['user_id'], items=df['common_identifier'])\n",
    "(interactions, _) = dataset.build_interactions(((row['user_id'], row['common_identifier']) for index, row in df.iterrows()))\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train, test = random_train_test_split(interactions, test_percentage=0.2, random_state=42)\n",
    "\n",
    "# Build the model\n",
    "model = LightFM(loss='warp')  # You can try different loss functions (e.g., 'warp', 'logistic', 'bpr')\n",
    "\n",
    "# Train the model\n",
    "model.fit(train, epochs=30, num_threads=2)\n",
    "\n",
    "# Evaluate the model\n",
    "precision = precision_at_k(model, test, k=20).mean()\n",
    "print(f\"Precision at k=20: {precision}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "precision at k=5,The higher the precision, the better the model is at suggesting relevant items within the top-k recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color: green;\">  LightFM model to get  top 5 recommendation of books for a user</span>\n",
    "\n",
    "**<u><span style=\"color: red;\"> Issue:</span></u>**\n",
    "\n",
    "Recommends same book 5 times instead of 5 different books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from lightfm import LightFM\n",
    "from lightfm.data import Dataset\n",
    "from lightfm.cross_validation import random_train_test_split\n",
    "from lightfm.evaluation import precision_at_k\n",
    "import numpy as np\n",
    "\n",
    "# Create a LightFM dataset\n",
    "dataset = Dataset()\n",
    "dataset.fit(users=df['user_id'], items=df['common_identifier'])\n",
    "#build user-item interactions where each row represents a user, each column represents an item\n",
    "(interactions, _) = dataset.build_interactions(((row['user_id'], row['common_identifier']) for index, row in df.iterrows()))\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train, test = random_train_test_split(interactions, test_percentage=0.2, random_state=42)\n",
    "\n",
    "# Build the model\n",
    "model = LightFM(loss='warp')  # You can try different loss functions (e.g., 'warp', 'logistic', 'bpr')\n",
    "\n",
    "# Train the model\n",
    "model.fit(train, epochs=30, num_threads=2)\n",
    "\n",
    "# Recommend top 5 books for a random user\n",
    "num_users, num_items = interactions.shape\n",
    "\n",
    "# Generate a random user ID\n",
    "random_user_id = np.random.randint(0, num_users)\n",
    "\n",
    "# Use the predict method to get scores for all items for the random user\n",
    "scores = model.predict(random_user_id, np.arange(num_items))\n",
    "\n",
    "# Get the indices of the top 5 items\n",
    "top_item_indices = sorted(range(num_items), key=lambda x: scores[x], reverse=True)[:5]\n",
    "\n",
    "# Map item indices to book titles\n",
    "top_item_titles = df.loc[df['common_identifier'].isin(top_item_indices), 'book_title'].tolist()\n",
    "\n",
    "# Print or use the top 5 book titles as recommendations for the random user\n",
    "#print(f\"Top 5 recommendations for user {random_user_id}: {top_item_titles}\")\n",
    "\n",
    "print(f\"Top 5 recommendations for user {random_user_id}: are as below\")\n",
    "for i, title in enumerate(top_item_titles[:5], start=1):\n",
    "    print(f\"{i}. {title}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  <span style=\"color: green;\"> Recommend books popular yearly </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['year_of_publication'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by year and calculate the average rating and number of ratings for each book\n",
    "yearly_stats = df.groupby(['year_of_publication', 'isbn']).agg({'book_rating': ['mean', 'count']}).reset_index()\n",
    "yearly_stats.columns = ['year', 'isbn', 'avg_rating', 'num_ratings']\n",
    "\n",
    "# Sort books within each year by the number of ratings and average rating\n",
    "yearly_stats = yearly_stats.sort_values(['year', 'num_ratings', 'avg_rating'], ascending=[True, False, False])\n",
    "\n",
    "# Function to recommend popular books for a given year\n",
    "def recommend_popular_books(year, top_n=5):\n",
    "    year_books = yearly_stats[yearly_stats['year'] == year].head(top_n)\n",
    "    recommended_books = df[df['isbn'].isin(year_books['isbn'])][['book_title', 'book_author']].drop_duplicates()\n",
    "    return recommended_books\n",
    "\n",
    "# Example usage\n",
    "year_to_recommend = 2004  # Replace with the desired year\n",
    "top_n_recommendations = 5  # Adjust the number of recommendations as needed\n",
    "\n",
    "print(f\"Popular Books in {year_to_recommend}:\\n\")\n",
    "popular_books = recommend_popular_books(year_to_recommend, top_n=top_n_recommendations)\n",
    "print(popular_books)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color: green;\">  Recommend books based on Author</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "# Assuming 'df' is your input DataFrame\n",
    "# Columns: ['book_title', 'book_author', 'year_of_publication', 'publisher', 'image_url_m', 'common_identifier', 'user_id', 'isbn', 'book_rating', 'age', 'city', 'country', 'user', 'item']\n",
    "\n",
    "# Create a TF-IDF vectorizer for book authors\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['book_author'])\n",
    "\n",
    "# Create a DataFrame to store the mapping between book_author and index in tfidf_matrix\n",
    "author_mapping = pd.DataFrame({'book_author': df['book_author'].unique(), 'index': range(len(df['book_author'].unique()))})\n",
    "\n",
    "# Function to recommend books by the same author\n",
    "def recommend_books_by_author(target_author, df=df, tfidf_matrix=tfidf_matrix, author_mapping=author_mapping):\n",
    "    # Filter books by the target author\n",
    "    author_books = df[df['book_author'] == target_author]['book_title'].unique()\n",
    "\n",
    "    # Get the index of the target author in the mapping\n",
    "    target_author_index = author_mapping[author_mapping['book_author'] == target_author]['index'].iloc[0]\n",
    "\n",
    "    # Calculate the similarity between books by the target author and all other books\n",
    "    target_author_tfidf = tfidf_matrix.getrow(target_author_index)\n",
    "    similarity_scores = linear_kernel(target_author_tfidf, tfidf_matrix).flatten()\n",
    "\n",
    "    # Sort books by similarity score in descending order\n",
    "    recommended_books = pd.DataFrame({'book_title': df['book_title'], 'similarity_score': similarity_scores})\n",
    "    recommended_books = recommended_books.sort_values(by='similarity_score', ascending=False)\n",
    "\n",
    "    return recommended_books.head(5)  # Return top 5 recommendations\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u><span style=\"color: red;\"> Issue:</span></u>**\n",
    "\n",
    "Some books repeated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling\n",
    "target_author_to_recommend = df['book_author'].sample().iloc[0]  # Randomly select a book author for recommendation\n",
    "\n",
    "print(f\"Recommendations for books by Author {target_author_to_recommend}:\\n\")\n",
    "book_recommendations = recommend_books_by_author(target_author_to_recommend)\n",
    "print(book_recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color: green;\"> Recommend book based on book title </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "# Create a TF-IDF vectorizer for book titles\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['book_title'])\n",
    "\n",
    "# Create a DataFrame to store the mapping between book_title and index in tfidf_matrix\n",
    "title_mapping = pd.DataFrame({'book_title': df['book_title'].unique(), 'index': range(len(df['book_title'].unique()))})\n",
    "\n",
    "# Function to recommend books by the same title\n",
    "def recommend_books_by_title(target_title, df=df, tfidf_matrix=tfidf_matrix, title_mapping=title_mapping):\n",
    "    # Filter books by the target title\n",
    "    title_books = df[df['book_title'] == target_title]['book_title'].unique()\n",
    "\n",
    "    # Get the index of the target title in the mapping\n",
    "    target_title_index = title_mapping[title_mapping['book_title'] == target_title]['index'].iloc[0]\n",
    "\n",
    "    # Calculate the similarity between books with the target title and all other books\n",
    "    target_title_tfidf = tfidf_matrix.getrow(target_title_index)\n",
    "    similarity_scores = linear_kernel(target_title_tfidf, tfidf_matrix).flatten()\n",
    "\n",
    "    # Sort books by similarity score in descending order\n",
    "    recommended_books = pd.DataFrame({'book_title': df['book_title'], 'similarity_score': similarity_scores})\n",
    "    \n",
    "    recommended_books = recommended_books.sort_values(by='similarity_score', ascending=False)\n",
    "\n",
    "    return recommended_books.head(5)  # Return top 5 recommendations\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u><span style=\"color: red;\"> Issue:</span></u>**\n",
    "\n",
    "Books repeated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling function\n",
    "target_title_to_recommend = df['book_title'].sample().iloc[0]  # Randomly select a book title for recommendation\n",
    "\n",
    "print(f\"Recommendations based on Book Title : {target_title_to_recommend}:\\n\")\n",
    "book_recommendations = recommend_books_by_title(target_title_to_recommend)\n",
    "print(book_recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model Evaluation: using other metrics than RSME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fraction of Concordant Pairs(fcp)\n",
    "FCP is a ranking-oriented metric that assesses the proportion of concordant pairs (i.e., pairs of user-item interactions where the predicted ranking order matches the actual ranking order).\n",
    "\n",
    "NOTE: Lower values for MAE and MSE indicate better accuracy, while higher values for FCP indicate better performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  1.1191\n",
      "MSE: 1.9287\n",
      "FCP:  0.5566\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.556563864282585"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model using additional metrics\n",
    "accuracy.mae(predictions)\n",
    "accuracy.mse(predictions)\n",
    "accuracy.fcp(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Error Analysis: Checking predictions versus actual ratings for few users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print some example predictions vs. actual ratings\n",
    "for prediction in predictions[:10]:\n",
    "    print(f\"User: {prediction.uid}, Book: {prediction.iid}, Predicted: {prediction.est}, Actual: {prediction.r_ui}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model Optimization: hyperparameter tuning to improve model performance. \n",
    "- Used grid search to find optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {'n_factors': [50, 100, 150], 'reg_all': [0.02, 0.05, 0.1]}\n",
    "\n",
    "# Create a grid search object\n",
    "grid_search = GridSearchCV(SVD, param_grid, measures=['rmse'], cv=3)\n",
    "\n",
    "# Fit the grid search object on the data\n",
    "grid_search.fit(data)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params['rmse']\n",
    "\n",
    "# Create a new SVD model with the best hyperparameters\n",
    "best_model = SVD(n_factors=best_params['n_factors'], reg_all=best_params['reg_all'])\n",
    "\n",
    "# Fit the best model on the training set\n",
    "best_model.fit(trainset)\n",
    "\n",
    "# Make predictions on the test set\n",
    "best_predictions = best_model.test(testset)\n",
    "\n",
    "# Evaluate the best model\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "accuracy.rmse(best_predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Error Analysis Report: actual and predicted book titles, and it indicates whether the prediction is considered accurate based on a threshold difference (in this case, ±2). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise.model_selection import GridSearchCV\n",
    "from surprise import Dataset, Reader\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import SVD\n",
    "from surprise import accuracy\n",
    "\n",
    "# Load data into the Surprise library's format\n",
    "reader = Reader(rating_scale=(1, 10))\n",
    "data = Dataset.load_from_df(df[['user_id', 'common_identifier', 'book_rating']], reader)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "trainset, testset = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {'n_factors': [50, 100, 150], 'reg_all': [0.02, 0.05, 0.1]}\n",
    "\n",
    "# Create a grid search object\n",
    "grid_search = GridSearchCV(SVD, param_grid, measures=['rmse'], cv=3)\n",
    "\n",
    "# Fit the grid search object on the data\n",
    "grid_search.fit(data)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params['rmse']\n",
    "\n",
    "# Create a new SVD model with the best hyperparameters\n",
    "best_model = SVD(n_factors=best_params['n_factors'], reg_all=best_params['reg_all'])\n",
    "\n",
    "# Fit the best model on the training set\n",
    "best_model.fit(trainset)\n",
    "\n",
    "# Make predictions on the test set\n",
    "best_predictions = best_model.test(testset)\n",
    "\n",
    "# Evaluate the best model\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "accuracy.rmse(best_predictions)\n",
    "\n",
    "# Error Analysis Report\n",
    "print(\"\\nError Analysis Report:\")\n",
    "for i, prediction in enumerate(best_predictions[:10], 1):\n",
    "    book_title_actual = df[df['common_identifier'] == prediction.iid]['book_title'].values[0]\n",
    "    book_title_predicted = df[df['common_identifier'] == prediction.iid]['book_title'].values[0]\n",
    "    \n",
    "    # Check if the prediction is accurate (within a threshold, e.g., ±1)\n",
    "    is_accurate = abs(prediction.est - prediction.r_ui) <= 2\n",
    "    \n",
    "    print(f\"\\nPrediction {i}:\")\n",
    "    print(f\"User ID: {prediction.uid}\")\n",
    "    print(f\"Actual Book Title: {book_title_actual}\")\n",
    "    print(f\"Predicted Book Title: {book_title_predicted}\")\n",
    "    print(f\"Predicted Rating: {prediction.est:.2f}\")\n",
    "    print(f\"Actual Rating: {prediction.r_ui}\")\n",
    "    print(f\"Is Accurate: {is_accurate}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "\n",
    "# Initialize TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Transform book titles into numerical vectors\n",
    "title_vectors = vectorizer.fit_transform(df['book_title'])\n",
    "\n",
    "# Initialize NearestNeighbors\n",
    "nn_model = NearestNeighbors(n_neighbors=4)  # n_neighbors=4 to include the input book itself\n",
    "\n",
    "# Fit the model using transformed vectors\n",
    "nn_model.fit(title_vectors)\n",
    "\n",
    "def recommend_books(input_book_title):\n",
    "    # Transform the input book title into a numerical vector\n",
    "    input_vector = vectorizer.transform([input_book_title])\n",
    "\n",
    "    # Get the indices of the nearest neighbors (including the input book itself)\n",
    "    _, neighbor_indices = nn_model.kneighbors(input_vector)\n",
    "\n",
    "    # Extract recommended book indices excluding the input book itself\n",
    "    neighbor_indices = neighbor_indices[0][1:]\n",
    "\n",
    "    # Get the recommended books based on the indices\n",
    "    recommended_books = df.iloc[neighbor_indices][['image_url_m', 'book_title', 'book_author']]\n",
    "\n",
    "    return recommended_books\n",
    "\n",
    "# Example usage\n",
    "input_title = 'Life of Pi'\n",
    "recommendations = recommend_books(input_title)\n",
    "print(f\"Recommendations for '{input_title}':\")\n",
    "print(recommendations[['image_url_m','book_title', 'book_author']])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
