{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Book-Oracle: Basic Recommendation System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Develop a basic Recommendation System\n",
    "- 26.11.2023\n",
    "- Janina, Oliwia, Neha, Nina"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "#Modelling\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, OneHotEncoder\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, roc_curve, confusion_matrix, make_scorer, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "#NLP\n",
    "import nltk\n",
    "\n",
    "#Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "plt.rcParams.update({ \"figure.figsize\" : (8, 5),\"axes.facecolor\" : \"white\", \"axes.edgecolor\":  \"black\"})\n",
    "plt.rcParams[\"figure.facecolor\"]= \"w\"\n",
    "pd.plotting.register_matplotlib_converters()\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "pd.options.display.float_format = \"{:,.2f}\".format\n",
    "\n",
    "RSEED = 42\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/kaggle_full_df.csv')\n",
    "df['country'].fillna('unknown', inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only Rating greater than 4\n",
    "df = df[df['book_rating']>0]\n",
    "\n",
    "#Only users from US or Canada\n",
    "df = df[df['country'].str.contains(\"usa|canada\")]\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add a new column with a total rating count for each book by common identifier\n",
    "df['rating_count'] = df.groupby(['book_title', 'book_author'])['book_rating'].transform('count')\n",
    "\n",
    "#Show a list of books that got the highest rating count, group by title and author to show unique books\n",
    "\n",
    "df.groupby(['book_title', 'book_author', 'rating_count']).size().reset_index(name='Count').sort_values(by='rating_count', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "popularity_threshold = 50\n",
    "df = df[df['rating_count'] >= popularity_threshold]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: White;\">Collaborative Filtering - Item based:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-surprise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Library used - Surprise\n",
    "\n",
    "- Model - matrix factorization SVD\n",
    "\n",
    "- Recommend top 5 books for a user. (Here, user_id is to be given as input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Identify books the user hasn't interacted with.\n",
    "- Make predictions for these books.\n",
    "- Sort predictions by estimated rating.\n",
    "- Extract the top N recommendations and return their titles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color: green;\"> Recommend top 5 books to User based on the books NOT interacted with </span>\n",
    "\n",
    "- Model used - Matrix Factorization SVD\n",
    "- Library used - Surprise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from surprise import Dataset, Reader\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import SVD\n",
    "from surprise import accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load data into the Surprise library's format\n",
    "reader = Reader(rating_scale=(1, 10))\n",
    "#reader = Reader(rating_scale=(1, 10))\n",
    "data = Dataset.load_from_df(df[['user_id', 'common_identifier', 'book_rating']], reader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split the data into training and testing sets\n",
    "trainset, testset = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build and train the matrix factorization model\n",
    "model = SVD()\n",
    "model.fit(trainset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Make predictions on the test set\n",
    "predictions = model.test(testset)\n",
    "\n",
    "# Evaluate the model's accuracy\n",
    "accuracy.rmse(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get book recommendations for a user\n",
    "def get_book_recommendations(user_id, df, model, n=5):\n",
    "    # Get the unique books the user hasn't interacted with\n",
    "    books_not_interacted = df[~df['common_identifier'].isin(df[df['user_id'] == user_id]['common_identifier'].tolist())]['common_identifier'].unique()\n",
    "\n",
    "    # Make predictions for the books the user hasn't interacted with\n",
    "    predictions = [model.predict(user_id, book) for book in books_not_interacted]\n",
    "\n",
    "    # Sort predictions by estimated rating in descending order\n",
    "    sorted_predictions = sorted(predictions, key=lambda x: x.est, reverse=True)\n",
    "\n",
    "    # Get the top N unique recommendations\n",
    "    top_n_recommendations = []\n",
    "    seen_books = set()\n",
    "    \n",
    "\n",
    "    for prediction in sorted_predictions:\n",
    "        if len(top_n_recommendations) >= n:\n",
    "            break\n",
    "\n",
    "        book_id = prediction.iid\n",
    "\n",
    "        # Check if the book has been recommended before\n",
    "        if book_id not in seen_books:\n",
    "            seen_books.add(book_id)\n",
    "            print(seen_books)\n",
    "\n",
    "            # Extract information from recommendations\n",
    "            book_info = df[df['common_identifier'] == book_id][['common_identifier', 'book_title', 'book_author', 'book_rating', 'image_url_m']].to_dict(orient='records')\n",
    "            top_n_recommendations.append(book_info[0])\n",
    "\n",
    "    return top_n_recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is your DataFrame and model is your collaborative filtering model\n",
    "\n",
    "# Example usage\n",
    "user_id = 31315\n",
    "recommendations = get_book_recommendations(user_id, df, model, n=5)\n",
    "\n",
    "# Print or use the recommendations as needed\n",
    "for i, book_info in enumerate(recommendations, start=1):\n",
    "    print(f\"{i}. Common Identifier: {book_info['common_identifier']}\")\n",
    "    print(f\"   Book Title: {book_info['book_title']}\")\n",
    "    print(f\"   Book Author: {book_info['book_author']}\")\n",
    "    print(f\"   Book Rating: {book_info['book_rating']}\")\n",
    "    print(f\"   Image URL (Medium): {book_info['image_url_m']}\")\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['user_id'].head(2000).unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  <span style=\"color: pink;\"> Accessing values from \"data\" object used in above code \n",
    "data = Dataset.load_from_df(df[['user_id', 'common_identifier', 'book_rating']], reader) </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output Format ----->   'user_id', 'common_identifier', 'book_rating']\n",
    "\n",
    "# Get the full training set from the data object\n",
    "full_trainset = data.build_full_trainset()\n",
    "\n",
    "# Convert the generator to a list and access the first few raw ratings\n",
    "raw_ratings = list(full_trainset.all_ratings())[:3]\n",
    "\n",
    "# Get the number of ratings (number of rows in the training set)\n",
    "num_ratings = full_trainset.n_ratings\n",
    "\n",
    "\n",
    "# Display the first few raw ratings\n",
    "print(raw_ratings)\n",
    "\n",
    "# Display the number of ratings\n",
    "print(\"Number of Ratings:\", num_ratings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#determine the number of unique users and items (common_identifiers) \n",
    "\n",
    "# Get the full training set from the data object\n",
    "full_trainset = data.build_full_trainset()\n",
    "\n",
    "# Get the number of users and items (common_identifiers)\n",
    "num_users = full_trainset.n_users\n",
    "num_items = full_trainset.n_items\n",
    "\n",
    "# Display the number of users and items\n",
    "print(\"Number of unique users:\", num_users)\n",
    "print(\"Number of items (common_identifiers):\", num_items)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lightfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install implicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from lightfm import LightFM\n",
    "from lightfm.data import Dataset\n",
    "from lightfm.cross_validation import random_train_test_split\n",
    "from lightfm.evaluation import precision_at_k\n",
    "\n",
    "# Create a LightFM dataset\n",
    "dataset = Dataset()\n",
    "dataset.fit(users=df['user_id'], items=df['common_identifier'])\n",
    "(interactions, _) = dataset.build_interactions(((row['user_id'], row['common_identifier']) for index, row in df.iterrows()))\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train, test = random_train_test_split(interactions, test_percentage=0.2, random_state=42)\n",
    "\n",
    "# Build the model\n",
    "model = LightFM(loss='warp')  # You can try different loss functions (e.g., 'warp', 'logistic', 'bpr')\n",
    "\n",
    "# Train the model\n",
    "model.fit(train, epochs=30, num_threads=2)\n",
    "\n",
    "# Evaluate the model\n",
    "precision = precision_at_k(model, test, k=20).mean()\n",
    "print(f\"Precision at k=20: {precision}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "precision at k=5,The higher the precision, the better the model is at suggesting relevant items within the top-k recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color: green;\">  Recommend books based on Author</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "# Assuming 'df' is your input DataFrame\n",
    "# Columns: ['book_title', 'book_author', 'year_of_publication', 'publisher', 'image_url_m', 'common_identifier', 'user_id', 'isbn', 'book_rating', 'age', 'city', 'country', 'user', 'item']\n",
    "\n",
    "# Create a TF-IDF vectorizer for book authors\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['book_author'])\n",
    "\n",
    "# Create a DataFrame to store the mapping between book_author and index in tfidf_matrix\n",
    "author_mapping = pd.DataFrame({'book_author': df['book_author'].unique(), 'index': range(len(df['book_author'].unique()))})\n",
    "\n",
    "# Function to recommend books by the same author\n",
    "def recommend_books_by_author(target_author, df=df, tfidf_matrix=tfidf_matrix, author_mapping=author_mapping):\n",
    "    # Filter books by the target author\n",
    "    author_books = df[df['book_author'] == target_author]['book_title'].unique()\n",
    "\n",
    "    # Get the index of the target author in the mapping\n",
    "    target_author_index = author_mapping[author_mapping['book_author'] == target_author]['index'].iloc[0]\n",
    "\n",
    "    # Calculate the similarity between books by the target author and all other books\n",
    "    target_author_tfidf = tfidf_matrix.getrow(target_author_index)\n",
    "    similarity_scores = linear_kernel(target_author_tfidf, tfidf_matrix).flatten()\n",
    "\n",
    "    # Sort books by similarity score in descending order\n",
    "    recommended_books = pd.DataFrame({'book_title': df['book_title'], 'similarity_score': similarity_scores})\n",
    "    recommended_books = recommended_books.sort_values(by='similarity_score', ascending=False)\n",
    "\n",
    "    return recommended_books.head(5)  # Return top 5 recommendations\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling\n",
    "target_author_to_recommend = df['book_author'].sample().iloc[0]  # Randomly select a book author for recommendation\n",
    "\n",
    "print(f\"Recommendations for books by Author {target_author_to_recommend}:\\n\")\n",
    "book_recommendations = recommend_books_by_author(target_author_to_recommend)\n",
    "print(book_recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model Evaluation: using other metrics than RSME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fraction of Concordant Pairs(fcp)\n",
    "FCP is a ranking-oriented metric that assesses the proportion of concordant pairs (i.e., pairs of user-item interactions where the predicted ranking order matches the actual ranking order).\n",
    "\n",
    "NOTE: Lower values for MAE and MSE indicate better accuracy, while higher values for FCP indicate better performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model using additional metrics\n",
    "accuracy.mae(predictions)\n",
    "accuracy.mse(predictions)\n",
    "accuracy.fcp(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Error Analysis: Checking predictions versus actual ratings for few users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print some example predictions vs. actual ratings\n",
    "for prediction in predictions[:5]:\n",
    "    print(f\"User: {prediction.uid}, Book: {prediction.iid}, Predicted: {prediction.est}, Actual: {prediction.r_ui}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model Optimization: hyperparameter tuning to improve model performance. \n",
    "- Used grid search to find optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {'n_factors': [50, 100, 150], 'reg_all': [0.02, 0.05, 0.1]}\n",
    "\n",
    "# Create a grid search object\n",
    "grid_search = GridSearchCV(SVD, param_grid, measures=['rmse'], cv=3)\n",
    "\n",
    "# Fit the grid search object on the data\n",
    "grid_search.fit(data)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params['rmse']\n",
    "\n",
    "# Create a new SVD model with the best hyperparameters\n",
    "best_model = SVD(n_factors=best_params['n_factors'], reg_all=best_params['reg_all'])\n",
    "\n",
    "# Fit the best model on the training set\n",
    "best_model.fit(trainset)\n",
    "\n",
    "# Make predictions on the test set\n",
    "best_predictions = best_model.test(testset)\n",
    "\n",
    "# Evaluate the best model\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "accuracy.rmse(best_predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Error Analysis Report: actual and predicted book titles, and it indicates whether the prediction is considered accurate based on a threshold difference (in this case, ±2). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise.model_selection import GridSearchCV\n",
    "from surprise import Dataset, Reader\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import SVD\n",
    "from surprise import accuracy\n",
    "\n",
    "# Load data into the Surprise library's format\n",
    "reader = Reader(rating_scale=(1, 10))\n",
    "data = Dataset.load_from_df(df[['user_id', 'common_identifier', 'book_rating']], reader)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "trainset, testset = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {'n_factors': [50, 100, 150], 'reg_all': [0.02, 0.05, 0.1]}\n",
    "\n",
    "# Create a grid search object\n",
    "grid_search = GridSearchCV(SVD, param_grid, measures=['rmse'], cv=3)\n",
    "\n",
    "# Fit the grid search object on the data\n",
    "grid_search.fit(data)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params['rmse']\n",
    "\n",
    "# Create a new SVD model with the best hyperparameters\n",
    "best_model = SVD(n_factors=best_params['n_factors'], reg_all=best_params['reg_all'])\n",
    "\n",
    "# Fit the best model on the training set\n",
    "best_model.fit(trainset)\n",
    "\n",
    "# Make predictions on the test set\n",
    "best_predictions = best_model.test(testset)\n",
    "\n",
    "# Evaluate the best model\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "accuracy.rmse(best_predictions)\n",
    "\n",
    "# Error Analysis Report\n",
    "print(\"\\nError Analysis Report:\")\n",
    "for i, prediction in enumerate(best_predictions[:10], 1):\n",
    "    book_title_actual = df[df['common_identifier'] == prediction.iid]['book_title'].values[0]\n",
    "    book_title_predicted = df[df['common_identifier'] == prediction.iid]['book_title'].values[0]\n",
    "    \n",
    "    # Check if the prediction is accurate (within a threshold, e.g., ±1)\n",
    "    is_accurate = abs(prediction.est - prediction.r_ui) <= 2\n",
    "    \n",
    "    print(f\"\\nPrediction {i}:\")\n",
    "    print(f\"User ID: {prediction.uid}\")\n",
    "    print(f\"Actual Book Title: {book_title_actual}\")\n",
    "    print(f\"Predicted Book Title: {book_title_predicted}\")\n",
    "    print(f\"Predicted Rating: {prediction.est:.2f}\")\n",
    "    print(f\"Actual Rating: {prediction.r_ui}\")\n",
    "    print(f\"Is Accurate: {is_accurate}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using pickle library to save & load models for streamlit app use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Saving models in pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an iterator object with write permission - model.pkl\n",
    "with open('svd.pkl', 'wb') as files:\n",
    "    pickle.dump(model, files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Loading saved model in pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load saved model\n",
    "with open('svd.pkl' , 'rb') as f:\n",
    "    load_svd_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(load_svd_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
